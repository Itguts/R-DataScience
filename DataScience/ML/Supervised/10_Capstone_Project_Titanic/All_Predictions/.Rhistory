train <- read.csv (path_train, stringsAsFactors = F,na.strings=c("","NA"," "))
test <- read.csv (path_test, stringsAsFactors = F,na.strings=c("","NA"," "))
## checking the dimension
dim(train) ## 418 rows and eleven columns
dim(test)
## getting the structure
str(train)
str(test)
summary(train)
## getting the statistical summary
summary(train)
summary(test)
########################################################################
#Lazy Predictor
########################################################################
# Initialize a Survived column to 0
test$Survived <- 0
my_solution <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
# Check the row number - It should match with test row number - 418
nrow(my_solution)
setwd("D:/Study/R/Repo/DataScience/ML/10_Capstone_Project_Titanic/All_Predictions")
########################################################################
#Lazy Predictor
########################################################################
# Initialize a Survived column to 0
test$Survived <- 0
my_solution <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
# Check the row number - It should match with test row number - 418
nrow(my_solution)
#write the solution for submission
write.csv(my_solution, file = "Lazy_Predictor.csv", row.names = FALSE)
########################################################################
#First Predictor with Gender
##################################################################################################################
# Set Survived to 1 if Sex equals "female"
test$Survived [test$Sex == "female"] <- 1
my_solution <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
# Check the row number - It should match with test row number - 418
nrow(my_solution)
#write the solution for submission
write.csv(my_solution, file = "Gender_Model.csv", row.names = FALSE)
##################################################################################################################
#Titanic Survival Tutorial - Part 2 :Data Cleaning and preparation
##################################################################################################################
# Combine train and test data for Data Cleaning and Preparation
Full <- rbind(train,test)
# Structure of the Full data
str(Full)
##################################################################################################################
#Titanic Survival Tutorial - Part 2 :Data Cleaning and preparation
##################################################################################################################
# Combine train and test data for Data Cleaning and Preparation
dim(train)
dim(test)
Full <- rbind(train,test)
# Structure of the Full data
str(Full)
summary(Full)
# Survival rates in absolute numbers
table(Full$Survived)
# Survival rates in proportions
prop.table(table(Full$Survived))
#Data Type conversion
str(Full)
#Data Type conversion
## Pclass- Passenger class- First,second and third. So it should factored
str(Full)
Full$Pclass = as.factor(Full$Pclass)
#################################################################################
#get percentage of missing value of the attributes - Approach 2 (Function)
sapply(Full, function(df)
{
sum(is.na(df)==T)/length(df)
})
paste((sum(is.na(df)==T)/length(df))*100, "%")
(sum(is.na(df)==T)/length(df))*100
sum(is.na(df)==T)/length(df)*100
sum(is.na(df)==T)/length(df)
Full$Pclass = as.factor(Full$Pclass)
#################################################################################
#get percentage of missing value of the attributes - Approach 2 (Function)
sapply(Full, function(df)
{
sum(is.na(df)==T)/length(df)
})
##or
sapply(Full, function(df)
{
sum(is.na(df)==T)/length(df)
})
sum(is.na(df)==T)/length(df)
Full$Pclass = as.factor(Full$Pclass)
#################################################################################
#get percentage of missing value of the attributes - Approach 2 (Function)
sapply(Full, function(df)
{
sum(is.na(df)==T)/length(df)
})
##or
sapply(Full, function(df)
{
sum(is.na(df)==T)/length(df)*100
})
Full$Pclass = as.factor(Full$Pclass)
#################################################################################
#get percentage of missing value of the attributes - Approach 2 (Function)
sapply(Full, function(df)
{
sum(is.na(df)==T)/length(df)
})
##or
sapply(Full, function(df)
{
paste((sum(is.na(df)==T)/length(df))*100, "%")
})
#Approach - Amelia Package
#install.packages("Amelia")
library("Amelia")
#Approach - Amelia Package
#install.packages("Amelia")
library("Amelia")
#Approach - Amelia Package
install.packages("Amelia")
#Approach - Amelia Package
#install.packages("Amelia")
library("Amelia")
missmap(Full, main = "Missing Map")
#Approach - Amelia Package  This package show a kind of heat map
#install.packages("Amelia")
library("Amelia")
missmap(Full, main = "Missing Map")
# Missing Value Imputation - Age
Full$Age[is.na(Full$Age)] <- mean(Full$Age,na.rm=T)
sum(is.na(Full$Age))
# Missing Value Imputation - Embarked
table(Full$Embarked, useNA = "always")
## 2 NA record can be replace with the mode which is "S", Mode because it is categorical value
# Substitute the missing values with the mode value
Full$Embarked[is.na(Full$Embarked)] <- 'S'
sum(is.na(Full$Embarked))
table(Full$Embarked, useNA = "always")
# Missing Value Imputation - Fare
# Substitute the missing values with the average value
Full$Fare[is.na(Full$Fare)] <- mean(Full$Fare,na.rm=T)
sum(is.na(Full$Fare))
# Missing Value Imputation - Cabin
#Drop the variable as the missing value is more than 20%
Full$Cabin <- NULL
##or
##Full <- Full[-11]
sum(is.na(Full$Fare))
#Check again for NA
sapply(Full, function(df)
{
sum(is.na(df)==T)/length(df)
})
# Data Cleaning is done, now we will again split back the data into train and test
# Train test splitting - Why do we need it?
train_cleaned <- Full[1:891,]
test_cleaned <- Full[892:1309,]
##################################################################################################################
#Titanic Survival Tutorial - Part 3: Data Exploration on cleaned data
##################################################################################################################
##univariate EDA
##bivariate EDA
##multivariate EDA
##################################################################################################################
##univariate EDA
library(ggplot2)
#categorical variables
xtabs(~Survived,train_cleaned)
ggplot(train_cleaned) + geom_bar(aes(x=Survived))
ggplot(train_cleaned) + geom_bar(aes(x=Sex))
ggplot(train_cleaned) + geom_bar(aes(x=Pclass))
ggplot(train_cleaned) + geom_bar(aes(x=Survived))
#categorical variables
xtabs(~Survived,train_cleaned)
ggplot(train_cleaned) + geom_bar(aes(x=Survived))
ggplot(train_cleaned) + geom_bar(aes(x=Survived))
ggplot(train_cleaned) + geom_bar(aes(x=Sex))
#Male has more count
ggplot(train_cleaned) + geom_bar(aes(x=Pclass))
#numerical variables
ggplot(train_cleaned) + geom_histogram(aes(x=Fare),fill = "white", colour = "black")
ggplot(train_cleaned) + geom_boxplot(aes(x=factor(0),y=Fare)) + coord_flip()
#numerical variables
ggplot(train_cleaned) + geom_histogram(aes(x=Fare),fill = "white", colour = "black")
ggplot(train_cleaned) + geom_boxplot(aes(x=factor(0),y=Fare)) + coord_flip()
ggplot(train_cleaned) + geom_histogram(aes(x=Age),fill = "white", colour = "black")
ggplot(train_cleaned) + geom_boxplot(aes(x=factor(0),y=Age)) + coord_flip()
#####################################################################################
##bivariate EDA
#Cat-Cat relationships
xtabs(~Survived+Sex,train_cleaned)
ggplot(train_cleaned) + geom_bar(aes(x=Sex, fill=factor(Survived)))
xtabs(~Survived+Pclass,train_cleaned)
ggplot(train_cleaned) + geom_bar(aes(x=Pclass, fill=factor(Survived)) )
xtabs(~Survived+Embarked,train_cleaned)
ggplot(train_cleaned) + geom_bar(aes(x=Embarked, fill=factor(Survived)) )
#Num-Cat relationships
ggplot(train_cleaned) + geom_boxplot(aes(x = factor(Survived), y = Age))
ggplot(train_cleaned) + geom_histogram(aes(x = Age),fill = "white", colour = "black") + facet_grid(factor(Survived) ~ .)
ggplot(train_cleaned) + geom_boxplot(aes(x = factor(Survived), y = Fare))
ggplot(train_cleaned) + geom_histogram(aes(x = Fare),fill = "white", colour = "black") + facet_grid(factor(Survived) ~ .)
#####################################################################################
##multivariate EDA
xtabs(~factor(Survived)+Pclass+Sex,train_cleaned)
ggplot(train_cleaned) + geom_bar(aes(x=Sex, fill=factor(Survived))) + facet_grid(Pclass ~ .)
xtabs(~Survived+Embarked+Sex,train_cleaned)
ggplot(train_cleaned) + geom_bar(aes(x=Sex, fill=factor(Survived))) + facet_grid(Embarked ~ .)
# Engineered variable 1: Child
# Create the column child, and indicate whether child or no child
Full$Child <- NA
Full$Child[Full$Age < 18] <- 1
Full$Child[Full$Age >= 18] <- 0
str(Full$Child)
ggplot(Full) + geom_bar(aes(x=Child))
ggplot(Full) + geom_bar(aes(x=Child,fill=factor(Survived))
ggplot(Full) + geom_bar(aes(x=Child,fill=factor(Survived)))
ggplot(Full) + geom_bar(aes(x=Child,fill=factor(Survived)))
# Engineered variable 2: Title
# Extract the title - Mr, Mrs, Miss
Full$Title <- sapply(Full$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][2]})
Full$Name
name
name <- "Stengel, Mrs. Charles Emil Henry (Annie May Morris)"
name
name1 <- split(name,',')
name1
name1 <- strsplit(name, split='[,.]')
name1
typeof(name1)
name1[[1]][[2]]
# Engineered variable 2: Title
# Extract the title - Mr, Mrs, Miss
Full$Title <- sapply(Full$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][2]})
Full$Title <- sub(' ', '', Full$Title)  # Remove the white space or blank
table(Full$Title)
ggplot(Full) + geom_bar(aes(x=Title))
ggplot(Full) + geom_bar(aes(x=Title,fill=factor(Survived)))
# Combine small title groups
Full$Title[Full$Title %in% c('Mme', 'Mlle')] <- 'Mlle'
Full$Title[Full$Title %in% c('Capt', 'Don', 'Major', 'Sir')] <- 'Sir'
Full$Title[Full$Title %in% c('Dona', 'Lady', 'the Countess', 'Jonkheer')] <- 'Lady'
# Convert to a factor
Full$Title <- factor(Full$Title)
table(Full$Title)
ggplot(Full) + geom_bar(aes(x=Title))
ggplot(Full) + geom_bar(aes(x=Title,fill=factor(Survived)))
# Engineered variable 3: Family size
Full$FamilySize <- Full$SibSp + Full$Parch + 1
table(Full$FamilySize)
ggplot(Full) + geom_bar(aes(x=FamilySize))
ggplot(Full) + geom_bar(aes(x=FamilySize,fill=factor(Survived)))
# Split back into test and train sets
train_Featured <- Full[1:891,]
test_Featured <- Full[892:1309,]
train_Featured$Survived <- as.factor(train_Featured$Survived)
train_Featured$Sex <- as.factor(train_Featured$Sex)
train_Featured$Embarked <- as.factor(train_Featured$Embarked)
test_Featured$Sex <- as.factor(test_Featured$Sex)
test_Featured$Embarked <- as.factor(test_Featured$Embarked)
head(train_Featured)
colnames(train_Featured)
colnames(train_Featured)
colnames(test_Featured)
##################################################################################################################
#Titanic Survival Tutorial - Part 5 : Logistics Regression
##################################################################################################################
################################################################################
# Step 1:Split data in train and test data
#install.packages("caTools")
library(caTools)
set.seed(390)
split <- sample.split(train_Featured, SplitRatio = 0.8)
split
train.data <- subset(train_Featured, split== "TRUE")
test.data <- subset(train_Featured, split== "FALSE")
str(train.data)
str(test.data)
# Model fitting - Try 1:without Feature Enginnering
#After removing Passenger ID, Name and Ticket, Child, Title. Family Size
logit_model1 <- glm(Survived ~ .,family=binomial(link='logit'),data = train.data[-c(1,4,9,12,13,14)])
# Model fitting - Try 1:without Feature Enginnering
#After removing Passenger ID, Name and Ticket, and featured columns- Child, Title. Family Size
logit_model1 <- glm(Survived ~ .,family=binomial(link='logit'),data = train.data[-c(1,4,9,12,13,14)])
summary(logit_model1) #AIC : 626.99
# Model fitting - Try 2 : With Feature Engineering
#After removing Passenger ID, Name and Ticket
logit_model2 <- glm(Survived ~ .,family=binomial(link='logit'),data = train.data[-c(1,4,9)])
summary(logit_model2) # AIC : 597 :  PClass, Age, SibSp, Parch
anova(logit_model1, logit_model2, test = "Chisq")
anova(logit_model2, test = 'Chisq')
#This anova test is with model2 and null deviation
#######################################################################################
# Step 3:Predict test data based on trained model -logit_model
fitted.results <- predict(logit_model2,newdata=test.data,type='response')
logit_model2
#This anova test is with model2 and null deviation
#######################################################################################
# Step 3:Predict test data based on trained model -logit_model
fitted.results <- predict(logit_model2,newdata=test.data,type='response')
train.data
# Model fitting - Try 2 : With Feature Engineering
#After removing Passenger ID, Name and Ticket
colnames(train.data)
train.data[13]
# Model fitting - Try 2 : With Feature Engineering
#After removing Passenger ID, Name and Ticket
#colnames(train.data)
#train.data[13]
logit_model2 <- glm(Survived ~ .,family=binomial(link='logit'),data = train.data[-c(1,4,9,13)])
summary(logit_model2) # AIC : 597 :  PClass, Age, SibSp, Parch
anova(logit_model1, logit_model2, test = "Chisq")
anova(logit_model2, test = 'Chisq')
#This anova test is with model2 and null deviation
#######################################################################################
# Step 3:Predict test data based on trained model -logit_model
fitted.results <- predict(logit_model2,newdata=test.data,type='response')
#######################################################################################
# Step 4: Change probabilities to class (0 or 1/Yes or No)
# If prob > 0.5 then 1, else 0. Threshold can be set for better results
fitted.results <- ifelse(fitted.results > 0.5,1,0)
fitted.results
fitted.results
#######################################################################################
# Step 4: Change probabilities to class (0 or 1/Yes or No)
# If prob > 0.5 then 1, else 0. Threshold can be set for better results
fitted.results <- ifelse(fitted.results > 0.5,1,0)
#######################################################################################
# Step 5: Evauate Model Accuracy using Confusion matrix
library(caret)
confusionMatrix(table(test.data$Survived, fitted.results))
accuracy <- mean(test.data$Survived!=fitted.results)
paste((1-accuracy),"accurate")
paste((1-accuracy)*100," % accurate")
paste(round((1-accuracy)*100)," % accurate")
paste(round((1-accuracy)*100),"% accurate")
# ROC-AUC Curve
#install.packages("ROCR")
library(ROCR)
ROCRPred <- prediction(fitted.results, test.data$Survived)
ROCRPerf <- performance(ROCRPred, measure ="tpr", x.measure ="fpr")
par(mfrow = c(1, 1))
plot(ROCRPerf, colorize = TRUE, print.cutoffs.at = seq(0.1,by=0.1),main = "ROC CURVE")
abline(a=0, b=1)
auc <- performance(ROCRPred, measure = "auc")
auc <- auc@y.values[[1]]
auc <- round(auc, 4)
legend (.6,.4,auc, title = "AUC", cex =1)
####################################################################################
# Make predictions on the test set
my_prediction <- predict(logit_model2, test_Featured, type = "response")
test_Featured
# If prob > 0.5 then 1, else 0. Threshold can be set for better results
my_prediction <- ifelse(my_prediction > 0.5,1,0)
# Finish the data.frame() call
my_solution <- data.frame(PassengerId = test_Featured$PassengerId, Survived = my_prediction)
# Use nrow() on my_solution - Should be 418
nrow(my_solution)
str(my_solution)
# Finish the write.csv() call
write.csv(my_solution, file = "Logistic_Regression.csv", row.names = FALSE)
##########################################################################################
logit_model3 <- glm(Survived ~ .,family=binomial(link='logit'),data = train_Featured[-c(1,4,9)])
summary(logit_model3)
my_prediction <- predict(logit_model3, test_Featured, type = "response")
my_prediction <- ifelse(my_prediction > 0.5,1,0)
my_solution <- data.frame(PassengerId = test_Featured$PassengerId, Survived = my_prediction)
nrow(my_solution)
write.csv(my_solution, file = "Logistic_Regression_Solution_Without_Feature.csv", row.names = FALSE)
# ROC-AUC Curve
#install.packages("ROCR")
library(ROCR)
ROCRPred <- prediction(fitted.results, test.data$Survived)
ROCRPerf <- performance(ROCRPred, measure ="tpr", x.measure ="fpr")
par(mfrow = c(1, 1))
plot(ROCRPerf, colorize = TRUE, print.cutoffs.at = seq(0.1,by=0.1),main = "ROC CURVE")
abline(a=0, b=1)
auc <- performance(ROCRPred, measure = "auc")
auc <- auc@y.values[[1]]
auc <- round(auc, 4)
legend (.6,.4,auc, title = "AUC", cex =1)
library(rattle)
##################################################################################################################
#Titanic Survival Tutorial - Part 6 : # Build Decision Tree Model
##################################################################################################################
install.packages('rattle')
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(rpart)
# Recreate the gender model
dtree1 <- rpart(Survived ~ Sex, data=train_Featured, method="class")
rpart.plot(dtree1)
fancyRpartPlot(dtree1)
my_Prediction <- predict(dtree1, test_Featured, type = "class")
my_solution <- data.frame(PassengerId = test_Featured$PassengerId, Survived = my_Prediction)
write.csv(my_solution, file = "Decision_Tree_Solution_by_Sex.csv", row.names = FALSE)
######################################################################################
#Build a deeper tree without feature enginnering
dtree2 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data=train_Featured,
method="class")
rpart.plot(dtree2)
fancyRpartPlot(dtree2)
my_Prediction <- predict(dtree2, test_Featured, type = "class")
my_solution <- data.frame(PassengerId = test_Featured$PassengerId, Survived = my_Prediction)
write.csv(my_solution, file = "Decision_Tree_Solution_Deeper_With_Feature.csv", row.names = FALSE)
######################################################################################
# Build a deeper tree - with feature engineering
dtree3 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Child +Title + FamilySize,
data=train_Featured,
method="class")
write.csv(my_solution, file = "Decision_Tree_Solution_Deeper_Without_Feature.csv", row.names = FALSE)
######################################################################################
# Build a deeper tree - with feature engineering
dtree3 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Child +Title + FamilySize,
data=train_Featured,
method="class")
rpart.plot(dtree3)
fancyRpartPlot(dtree3)
my_Prediction <- predict(dtree3, test_Featured, type = "class")
my_solution <- data.frame(PassengerId = test_Featured$PassengerId, Survived = my_Prediction)
write.csv(my_solution, file = "Decision_Tree_Solution_Deeper_With_Feature.csv", row.names = FALSE)
######################################################################################
# Go all over and reach max depth of the tree by making cp =0 and minsplit = 2, it will give overfit model
dtree4 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Child +Title + FamilySize,
data=train_Featured,
method="class",
control=rpart.control(minsplit=2, cp=0))
fancyRpartPlot(dtree4)
my_Prediction <- predict(dtree4, test_Featured, type = "class")
my_solution <- data.frame(PassengerId = test_Featured$PassengerId, Survived = my_Prediction)
write.csv(my_solution, file = "Decision_Tree_Solution_Overfit.csv", row.names = FALSE)
#Accuracy = 69% -> worst than gender model -> Welcome to overfitting
######################################################################################
# Let's handle Overfitting Problem
printcp(dtree4)
#Find the value of CP for which cross validation error is minimum
min(dtree4$cptable[,"xerror"])
which.min(dtree4$cptable[,"xerror"])
cpmin <- dtree4$cptable[9 "CP"]
cpmin
which.min(dtree4$cptable[,"xerror"])
cpmin <- dtree4$cptable[9, "CP"]
cpmin
#Prune the tree by setting the CP parameter as =  cpmin
dtree5 = prune(dtree4, cp = cpmin)
rpart.plot(dtree5)
fancyRpartPlot(dtree5)
my_Prediction <- predict(dtree5, test_Featured, type = "class")
my_solution <- data.frame(PassengerId = test_Featured$PassengerId, Survived = my_Prediction)
write.csv(my_solution, file = "Decision_Tree_Solution_Pruned.csv", row.names = FALSE)
#Alternative way of pruning tree
dtree5 <- prp(dtree4,snip=TRUE)$obj
fancyRpartPlot(dtree5)
my_Prediction <- predict(dtree5, test_Featured, type = "class")
my_solution <- data.frame(PassengerId = test_Featured$PassengerId, Survived = my_Prediction)
write.csv(my_solution, file = "Decision_Tree_Solution_Pruned_PRP.csv", row.names = FALSE)
write.csv(my_solution, file = "Decision_Tree_Solution_Pruned_PRP.csv", row.names = FALSE)
#Find the value of CP for which cross validation error is minimum
min(dtree4$cptable[,"xerror"])
which.min(dtree4$cptable[,"xerror"])
cpmin <- dtree4$cptable[9, "CP"]
cpmin
##################################################################################################################
#Titanic Survival Tutorial - Part 7 : # Build Random Forest Model
##################################################################################################################
set.seed(415)
library("randomForest")
fit <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Child + Title + FamilySize,
data=train_Featured, importance=TRUE, ntree=2000)
# Look at variable importance
varImpPlot(fit)
# Now let's make a prediction and write a submission file
my_Prediction <- predict(fit, test_Featured)
my_solution <- data.frame(PassengerId = test_Featured$PassengerId, Survived = my_Prediction)
write.csv(my_solution, file = "Featured_First_Random_Forest.csv", row.names = FALSE)
# Build condition inference tree Random Forest
library("caret")
#install.packages("party")
library("party")
install.packages("party")
# Build condition inference tree Random Forest
library("caret")
#install.packages("party")
library("party")
set.seed(415)
tree <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Child + Title + FamilySize,
data = train_Featured, controls=cforest_unbiased(ntree=2000, mtry=3))
#install.packages("party")
library("party")
install.packages("party")
library("party")
install.packages("multcomp")
library("party")
set.seed(415)
tree <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Child + Title + FamilySize,
data = train_Featured, controls=cforest_unbiased(ntree=2000, mtry=3))
# Now let's make a prediction and write a submission file
my_Prediction <- predict(fit, test_Featured, OOB=TRUE, type = "response")
my_solution <- data.frame(PassengerId = test_Featured$PassengerId, Survived = my_Prediction)
write.csv(my_solution, file = "Featured_RandomForest_Using_cforest.csv", row.names = FALSE)
# Fitting SVM to the Training set
# install.packages('e1071')
library(e1071)
svm_model<-svm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Child +Title + FamilySize, data=train_Featured, method = "class")
summary(svm_model)
my_Prediction <- predict(svm_model, test_Featured, type = "class")
my_solution <- data.frame(PassengerId = test_Featured$PassengerId, Survived = my_Prediction)
write.csv(my_solution, file = "SVM_Solution.csv", row.names = FALSE)
