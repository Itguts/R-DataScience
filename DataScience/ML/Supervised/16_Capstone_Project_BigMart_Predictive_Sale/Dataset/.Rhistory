# Item_Visibility vs Item_Outlet_Sales
p10 = ggplot(train) + geom_point(aes(Item_Visibility, Item_Outlet_Sales), colour = "violet", alpha = 0.3) +
theme(axis.title = element_text(size = 8.5))
p10
# Item_MRP vs Item_Outlet_Sales
p11 = ggplot(train) + geom_point(aes(Item_MRP, Item_Outlet_Sales), colour = "violet", alpha = 0.3) +
theme(axis.title = element_text(size = 8.5))
p11
Item_MRP vs Item_Outlet_Sales
# Item_MRP vs Item_Outlet_Sales
p11 = ggplot(train) + geom_point(aes(Item_MRP, Item_Outlet_Sales), colour = "violet", alpha = 0.3) +
theme(axis.title = element_text(size = 8.5))
p11
second_row_2 = plot_grid(p10, p11, ncol = 2)
plot_grid(p9, second_row_2, nrow = 2)
#-------------------------------------------------------------------------------------------------------------------------
# Item_Type vs Item_Outlet_Sales
p12 = ggplot(train) + geom_boxplot(aes(Item_Type, Item_Outlet_Sales), fill = "magenta") +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
axis.text = element_text(size = 6),
axis.title = element_text(size = 8.5))
Item_Type vs Item_Outlet_Sales
#-------------------------------------------------------------------------------------------------------------------------
# Item_Type vs Item_Outlet_Sales
p12 = ggplot(train) + geom_boxplot(aes(Item_Type, Item_Outlet_Sales), fill = "magenta") +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
axis.text = element_text(size = 6),
axis.title = element_text(size = 8.5))
p12
p12 = ggplot(train) + geom_violin(aes(Item_Type, Item_Outlet_Sales), fill = "magenta") +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
axis.text = element_text(size = 6),
axis.title = element_text(size = 8.5))
p12
# Item_Fat_Content vs Item_Outlet_Sales
p13 = ggplot(train) + geom_violin(aes(Item_Fat_Content, Item_Outlet_Sales), fill = "magenta") +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
axis.text = element_text(size = 8),
axis.title = element_text(size = 8.5))
p13
# Outlet_Identifier vs Item_Outlet_Sales
p14 = ggplot(train) + geom_violin(aes(Outlet_Identifier, Item_Outlet_Sales), fill = "magenta") +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
axis.text = element_text(size = 8),
axis.title = element_text(size = 8.5))
Outlet_Identifier vs Item_Outlet_Sales
# Outlet_Identifier vs Item_Outlet_Sales
p14 = ggplot(train) + geom_violin(aes(Outlet_Identifier, Item_Outlet_Sales), fill = "magenta") +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
axis.text = element_text(size = 8),
axis.title = element_text(size = 8.5))
p14
second_row_3 = plot_grid(p13, p14, ncol = 2)
plot_grid(p12, second_row_3, ncol = 1)
ggplot(train) + geom_violin(aes(Outlet_Size, Item_Outlet_Sales), fill = "magenta")
p15 = ggplot(train) + geom_violin(aes(Outlet_Location_Type, Item_Outlet_Sales), fill = "magenta")
p16 = ggplot(train) + geom_violin(aes(Outlet_Type, Item_Outlet_Sales), fill = "magenta")
plot_grid(p15, p16, ncol = 1)
missing_index = which(is.na(combi$Item_Weight))
#-------------------------------------------------------------------------------------------------------------------------
## Missing Value Treatment
colSums(is.na(combi))
missing_index = which(is.na(combi$Item_Weight))
for(i in missing_index){
item = combi$Item_Identifier[i]
combi$Item_Weight[i] = mean(combi$Item_Weight[combi$Item_Identifier == item], na.rm = T)
}
# replacing 0 in Item_Visibility with mean
zero_index = which(combi$Item_Visibility == 0)
for(i in zero_index){
item = combi$Item_Identifier[i]
combi$Item_Visibility[i] = mean(combi$Item_Visibility[combi$Item_Identifier == item], na.rm = T)
}
# create a new feature 'Item_Type_new'
perishable = c("Breads", "Breakfast", "Dairy", "Fruits and Vegetables", "Meat", "Seafood")
non_perishable = c("Baking Goods", "Canned", "Frozen Foods", "Hard Drinks", "Health and Hygiene",
"Household", "Soft Drinks")
colnames(combi)
combi$Item_Type_new = ifelse(combi$Item_Type %in% perishable, "perishable", ifelse(combi$Item_Type %in% non_perishable, "non_perishable", "not_sure"))
colnames(combi)
head(combi,10)
name(combi$Item_Identifier)
names(combi$Item_Identifier)
unique(combi$Item_Identifier)
unique(combi$Item_Type_new)
#combi$Item_Category = substr(combi$Item_Identifier,1,2)
#or
combi[,Item_category := substr(combi$Item_Identifier, 1, 2)]
unique(combi$Item_category)
unique(combi$Item_Fat_Content)
combi$Item_Fat_Content[combi$Item_category == "NC"] = "Non-Edible"
# years of operation of outlets
combi[,Outlet_Years := 2013 - Outlet_Establishment_Year]
combi$Outlet_Establishment_Year = as.factor(combi$Outlet_Establishment_Year)
ggplot(train) + geom_point(aes(Item_MRP, Item_Outlet_Sales), colour = "violet", alpha = 0.3)
# creating new independent variable - Item_MRP_clusters
Item_MRP_clusters = kmeans(combi$Item_MRP, centers = 4)
table(Item_MRP_clusters$cluster) # display no. of observations in each cluster
combi$Item_MRP_clusters = as.factor(Item_MRP_clusters$cluster)
#or group them manually
# combi[,Item_MRP_clusters := ifelse(Item_MRP < 69, "1st",
#                                    ifelse(Item_MRP >= 69 & Item_MRP < 136, "2nd",
#                                           ifelse(Item_MRP >= 136 & Item_MRP < 203, "3rd", "4th")))]
#-------------------------------------------------------------------------------------------------------------------------
## Label Encoding
unique(combi$Outlet_Size)
combi[,Outlet_Size_num := ifelse(Outlet_Size == "Small", 0,
ifelse(Outlet_Size == "Medium", 1, 2))]
unique(combi$Outlet_Location_Type)
unique(combi$Outlet_Location_Type)
combi[,Outlet_Location_Type_num := ifelse(Outlet_Location_Type == "Tier 3", 0,
ifelse(Outlet_Location_Type == "Tier 2", 1, 2))]
# removing categorical variables after label encoding
combi[, c("Outlet_Size", "Outlet_Location_Type") := NULL]
ohe = dummyVars("~.", data = combi[,-c("Item_Identifier", "Outlet_Establishment_Year", "Item_Type")], fullRank = T)
combi[,Outlet_Location_Type_num := ifelse(Outlet_Location_Type == "Tier 3", 0,
ifelse(Outlet_Location_Type == "Tier 2", 1, 2))]
# removing categorical variables after label encoding
combi[, c("Outlet_Size", "Outlet_Location_Type") := NULL]
ohe = dummyVars("~.", data = combi[,-c("Item_Identifier", "Outlet_Establishment_Year", "Item_Type")], fullRank = T)
ohe
ohe
ohe = dummyVars("~.", data = combi[,-c("Item_Identifier", "Outlet_Establishment_Year", "Item_Type")], fullRank = T)
ohe_df = data.table(predict(ohe, combi[,-c("Item_Identifier", "Outlet_Establishment_Year", "Item_Type")]))
head(ohe_df)
colnames(ohe_df)
dim(ohe_df)
dim(combi)
dim(combi) # 16 already columns
combi = cbind(combi[,"Item_Identifier"], ohe_df)
dim(combi)
combi[,"Item_Identifier"]
dim(combi)
colnames(combi)
#-------------------------------------------------------------------------------------------------------------------------
## Remove skewness
library(e1071)
ggplot(data=combi) + geom_bar(aes(x= combi$Item_Visibility))
ggplot(data=combi) + geom_histogram(aes(x= combi$Item_Visibility))
#right skewed
skewness(combi$Item_Visibility)
ggplot(data=combi) + geom_histogram(aes(x= combi$Item_Visibility))
combi[,Item_Visibility := log(Item_Visibility + 1)] # log + 1 to avoid division by zero
ggplot(data=combi) + geom_histogram(aes(x= combi$Item_Visibility))
skewness(combi$price_per_unit_wt)
combi[,price_per_unit_wt := log(price_per_unit_wt + 1)]
skewness(combi$price_per_unit_wt)
combi[,price_per_unit_wt := log(price_per_unit_wt + 1)]
# Price per unit weight
combi[,price_per_unit_wt := Item_MRP/Item_Weight]
#case2:-
ggplot(data=combi) + geom_histogram(aes(x= combi$))
#case2:-
ggplot(data=combi) + geom_histogram(aes(x= combi$price_per_unit_wt))
skewness(combi$price_per_unit_wt)
combi[,price_per_unit_wt := log(price_per_unit_wt + 1)]
ggplot(data=combi) + geom_histogram(aes(x= combi$price_per_unit_wt))
num_vars = which(sapply(combi, is.numeric)) # index of numeric features
num_vars_names = names(num_vars)
combi_numeric = combi[,setdiff(num_vars_names, "Item_Outlet_Sales"), with = F]
View(combi_numeric)
View(combi_numeric)
combi_numeric = combi[,setdiff(num_vars_names, "Item_Outlet_Sales"), with = F]
?preProcess
prep_num = preProcess(combi_numeric, method=c("center", "scale"))
combi_numeric_norm = predict(prep_num, combi_numeric)
combi_numeric
combi
combi_numeric_norm
combi_numeric_norm = predict(prep_num, combi_numeric)
combi[,setdiff(num_vars_names, "Item_Outlet_Sales") := NULL] # removing numeric independent variables
combi = cbind(combi, combi_numeric_norm)
combi
combi
## splitting data back to train and test
train = combi[1:nrow(train)]
test = combi[(nrow(train) + 1):nrow(combi)]
test[,Item_Outlet_Sales := NULL] # removing Item_Outlet_Sales as it contains only NA for test dataset
## Correlation Plot
cor_train = cor(train[,-c("Item_Identifier")])
corrplot(cor_train, method = "pie", type = "lower", tl.cex = 0.9)
linear_reg_mod = lm(Item_Outlet_Sales ~ ., data = train[,-c("Item_Identifier")])
summary(linear_reg_mod)
#lm with onyl higly correlated variables
linear_reg_mod2 = lm(Item_Outlet_Sales ~ Item_MRP+Outlet_IdentifierOUT013+Outlet_IdentifierOUT017+Outlet_IdentifierOUT018+Outlet_IdentifierOUT027+Outlet_IdentifierOUT035+Outlet_IdentifierOUT045+Outlet_IdentifierOUT046+Outlet_IdentifierOUT049, data = train[,-c("Item_Identifier")])
summary(linear_reg_mod2)
## predicting on test set and writing a submission file
submission$Item_Outlet_Sales = predict(linear_reg_mod2, test[,-c("Item_Identifier")])
write.csv(submission, "Linear_Reg_submit.csv", row.names = F)
#-------------------------------------------------------------------------------------------------------------------------
## Lasso Regression
set.seed(1235)
my_control = trainControl(method="cv", number=5)
Grid = expand.grid(alpha = 1, lambda = seq(0.001,0.1,by = 0.0002))
lasso_linear_reg_mod = train(x = train[, -c("Item_Identifier", "Item_Outlet_Sales")], y = train$Item_Outlet_Sales,
method='glmnet', trControl= my_control, tuneGrid = Grid)
## splitting data back to train and test
train = combi[1:nrow(train)]
test = combi[(nrow(train) + 1):nrow(combi)]
test[,Item_Outlet_Sales := NULL] # removing Item_Outlet_Sales as it contains only NA for test dataset
## predicting on test set and writing a submission file
submission$Item_Outlet_Sales = predict(linear_reg_mod2, test[,-c("Item_Identifier")])
## splitting data back to train and test
train = combi[1:nrow(train)]
test = combi[(nrow(train) + 1):nrow(combi)]
test[,Item_Outlet_Sales := NULL] # removing Item_Outlet_Sales as it contains only NA for test dataset
#-------------------------------------------------------------------------------------------------------------------------
## Lasso Regression
set.seed(1235)
my_control = trainControl(method="cv", number=5)
Grid = expand.grid(alpha = 1, lambda = seq(0.001,0.1,by = 0.0002))
lasso_linear_reg_mod = train(x = train[, -c("Item_Identifier", "Item_Outlet_Sales")], y = train$Item_Outlet_Sales,
method='glmnet', trControl= my_control, tuneGrid = Grid)
## splitting data back to train and test
train = combi[1:nrow(train)]
test = combi[(nrow(train) + 1):nrow(combi)]
test[,Item_Outlet_Sales := NULL] # removing Item_Outlet_Sales as it contains only NA for test dataset
#-------------------------------------------------------------------------------------------------------------------------
## RandomForest Model
set.seed(1237)
my_control = trainControl(method="cv", number=5)
tgrid = expand.grid(
.mtry = c(3:10),
.splitrule = "variance",
.min.node.size = c(10,15,20)
)
rf_mod = train(x = train[, -c("Item_Identifier", "Item_Outlet_Sales")],
y = train$Item_Outlet_Sales,
method='ranger',
trControl= my_control,
tuneGrid = tgrid,
num.trees = 400,
importance = "permutation")
#-------------------------------------------------------------------------------------------------------------------------
## RandomForest Model
set.seed(1237)
my_control = trainControl(method="cv", number=5)
tgrid = expand.grid(
.mtry = c(3:10),
.splitrule = "variance",
.min.node.size = c(10,15,20)
)
rf_mod = train(x = train[, -c("Item_Identifier", "Item_Outlet_Sales")],
y = train$Item_Outlet_Sales,
method='ranger',
trControl= my_control,
tuneGrid = tgrid,
num.trees = 400,
importance = "permutation")
# mean validation score
mean(rf_mod$resample$RMSE)
param_list = list(
objective = "reg:linear",
eta=0.01,
gamma = 1,
max_depth=6,
subsample=0.8,
colsample_bytree=0.5
)
## converting train and test into xgb.DMatrix format
dtrain = xgb.DMatrix(data = as.matrix(train[,-c("Item_Identifier", "Item_Outlet_Sales")]), label= train$Item_Outlet_Sales)
dtest = xgb.DMatrix(data = as.matrix(test[,-c("Item_Identifier")]))
## 5-fold cross-validation to find optimal value of nrounds
set.seed(112)
xgbcv = xgb.cv(params = param_list,
data = dtrain,
nrounds = 1000,
nfold = 5,
print_every_n = 10,
early_stopping_rounds = 30,
maximize = F)
## training XGBoost model at nrounds = 428
xgb_model = xgb.train(data = dtrain, params = param_list, nrounds = 428)
## Variable Importance
var_imp = xgb.importance(feature_names = setdiff(names(train), c("Item_Identifier", "Item_Outlet_Sales")),
model = xgb_model)
xgb.plot.importance(var_imp)
## training XGBoost model at nrounds = 428
xgb_model = xgb.train(data = dtrain, params = param_list, nrounds = 439)
## Variable Importance
var_imp = xgb.importance(feature_names = setdiff(names(train), c("Item_Identifier", "Item_Outlet_Sales")),
model = xgb_model)
xgb.plot.importance(var_imp)
##Load data
mtcars
##Load data
dataset <- mtcars
dim(mtcars)
colnames(mtcars)
str(mtcars)
summary(mtcars)
str(mtcars)
corrdata <- cor(mtcars)
corrplot(corrdata)
dataset <- dataset %<%
select(cyl,disp,hp,wt,mpg)
## Analysis
#1. cyl in highly negative correlated to mpg
#2. same disp,hp,wt
##Lets go with these 4 features only
library(dplyr)
dataset <- dataset %>%
select(cyl,disp,hp,wt,mpg)
dim(dataset)
head(dataset,5)
colnames(dataset)
library(caTools)
set.seed(123)
split <- sample.split(dataset,SplitRatio = 0.8)
train <- subset(dataset,split=="TRUE")
test <- subset(dataset,split=="FALSE")
dim(train)
dim(test)
# build a linear regression model
linearmodel <- lm(data = train,x = mpg,y = c(cyl,disp,hp,wt))
# build a linear regression model
linearmodel <- lm(data = train,x = train$mpg,y = c(train$cyl,train$disp,train$hp,train$wt))
summary(linearmodel)
#or
linearmodel <- lm(mpg~., data=dataset)
summary(linearmodel)
# build a linear regression model
#linearmodel <- lm(data = train,x = train$mpg,y = c(train$cyl,train$disp,train$hp,train$wt))
#or
linear_reg_model <- lm(mpg~., data=dataset)
summary(linear_reg_model)
###############################################################################
# Multiple Regression
x1 <- c(45, 41, 47, 39, 41, 43)
x2 <- c(27,27,25,24,22,23)
x3 <- c(42,37,37,28,18,18)
y <- c(90,82,94,76,83,85)
score <- data.frame(x1,x2,x3,y)
str(score)
head(score)
lm.out <- lm(y~x1, data = score)
summary(lm.out)
# Predict
predicted <- predict(lm.out, data = score, type = "response")
predicted
score
## test the model
test
## test the model
test[-mpg]
## test the model
test[,-mpg]
## test the model
test[,"-mpg"]
#install.packages("arules")
# Load the libraries
library(arules)
## test the model
test[,-4]
## test the model
test[,-3]
## test the model
test[,-5]
my_prediction <- predict(linear_reg_mod,test[,-5],type="response")
summary(linear_reg_model)
## test the model
test[,-5]
my_prediction <- predict(linear_reg_mod, test[,-5],type="response")
my_prediction <- predict(linear_reg_mod, test,type="response")
# build a linear regression model
#linearmodel <- lm(data = train,x = train$mpg,y = c(train$cyl,train$disp,train$hp,train$wt))
#or
linear_Reg_model <- lm(mpg~., data=dataset)
summary(linear_Reg_model)
my_prediction <- predict(linear_Reg_mod, test,type="response")
# build a linear regression model
#linearmodel <- lm(data = train,x = train$mpg,y = c(train$cyl,train$disp,train$hp,train$wt))
#or
linear_Reg_model <- lm(mpg~., data=dataset)
summary(linear_Reg_model)
## test the model
test[,-5]
my_prediction <- predict(linear_Reg_mod, test,type="response")
dataset <- mtcars
dim(mtcars)
colnames(mtcars)
str(mtcars)
summary(mtcars)
## generate correlation plot and fetch only relevant feature for modelling and predicting the mpg- mileage of the car
# based on those paramters/features or independent variables
corrdata <- cor(mtcars)
corrplot(corrdata)
## Analysis
#1. cyl in highly negative correlated to mpg
#2. same disp,hp,wt
##Lets go with these 4 features only
library(dplyr)
dataset <- dataset %>%
select(cyl,disp,hp,wt,mpg)
dim(dataset)
head(dataset,5)
colnames(dataset)
#Split into train and test using catool library
library(caTools)
set.seed(123)
split <- sample.split(dataset,SplitRatio = 0.8)
train <- subset(dataset,split=="TRUE")
test <- subset(dataset,split=="FALSE")
dim(train)
dim(test)
# build a linear regression model
#linearmodel <- lm(data = train,x = train$mpg,y = c(train$cyl,train$disp,train$hp,train$wt))
#or
linear_Reg_model <- lm(mpg~., data=dataset)
summary(linear_Reg_model)
my_prediction <- predict(linear_Reg_model, test,type="response")
##evaluation
sqrt(mean((my_prediction - test$mpg)^2)) #RMSE
my_prediction
test$mpg
actual
# take first 20 rows to train our model
model <- lm(mpg ~ hp, mtcars[1:20, ])
summary(model)
# Predict in-sample
predicted <- predict(model, mtcars[1:20, ], type = "response")
predicted
mtcars$mpg
# Calculate RMSE
actual <- mtcars[1:20, "mpg"]
actual
sqrt(mean((predicted - actual)^2)) #RMSE
## For Azure deployment
#step1- create a test schema for input of the web api
testSchema <- subset(test,select=c(-mpg))
testSchema
str(testSchema )
## For Azure deployment
#step1- create a test schema for input of the web api
testSchema <- subset(test,select=c(-mpg))
str(testSchema )
Predict_Mileage <- function(testdata)  {
predictions <- predict(linear_Reg_model, testdata, type='response')
output <- data.frame(testdata,ScoredLabeles = predictions)
output
}
#test the scoring function
Predict_Mileage(test)
ws <- workspace(id='fbd4cd8fc69f458c902949d5a4f6bfe7',
auth='M9h6DqXJiijBJOu8MS1tkmaLqmUFaJJoZOyLzKBUDLyVLPXu7LWoeWF3LLR6zN1ZF9rK8mEnWyXLV6MFZlUUOA==',
api_endpint='https://studioapi/azureml.net'
)
#step3 - Connect to Azure ML
# login to Azure ML
# https://studio.azureml.net
# initialize workspace to connect to azure ML studion
#ws <- workspace(id='workspaceid',
#               auth='authenticaltoken',
#              api_endpint='https://studioapi/azureml.net'
#             )
## you can get all above information in the setting of your azure ML studio dasboard after login
install.packages("AzureML")
install.packages("devtools")
library(AzureML)
library(devtools)
ws <- workspace(id='fbd4cd8fc69f458c902949d5a4f6bfe7',
auth='M9h6DqXJiijBJOu8MS1tkmaLqmUFaJJoZOyLzKBUDLyVLPXu7LWoeWF3LLR6zN1ZF9rK8mEnWyXLV6MFZlUUOA==',
api_endpint='https://studioapi/azureml.net'
)
test
## For Azure deployment
#step1- create a test schema for input of the web api
carSchema <- subset(test,select=c(-mpg))
str(carSchema )
# now deploy my function -Predict_Mileage as a web service
# publish function
myapi <- publishWebService(ws = ws,fun = Predict_Mileage,
name = 'Predict New car Mileage',
inputSchema = carSchema,
data.frame = TRUE
)
# login to Azure ML
# https://studio.azureml.net
# initialize workspace to connect to azure ML studion
#ws <- workspace(id='workspaceid',
#               auth='authenticaltoken',
#              api_endpint='https://studioapi/azureml.net'
#             )
## you can get all above information in the setting of your azure ML studio dasboard after login
#install.packages("AzureML")
#install.packages("devtools")
install.packages("zip")
# now deploy my function -Predict_Mileage as a web service
# publish function
myapi <- publishWebService(ws = ws,fun = Predict_Mileage,
name = 'Predict New car Mileage',
inputSchema = carSchema,
data.frame = TRUE
)
# https://studio.azureml.net
# initialize workspace to connect to azure ML studion
#ws <- workspace(id='workspaceid',
#               auth='authenticaltoken',
#              api_endpint='https://studioapi/azureml.net'
#             )
## you can get all above information in the setting of your azure ML studio dasboard after login
#install.packages("AzureML")
#install.packages("devtools")
#install.packages("zip")
library(zip)
# now deploy my function -Predict_Mileage as a web service
# publish function
myapi <- publishWebService(ws = ws,fun = Predict_Mileage,
name = 'Predict New car Mileage',
inputSchema = carSchema,
data.frame = TRUE
)
